
import json
import os
import uuid
import re
import asyncio
from typing import List, Optional, Dict
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import Message, InlineKeyboardButton, InlineKeyboardMarkup
from aiogram.utils.keyboard import InlineKeyboardBuilder
import logging

import hashlib
import re
import time
import json
import os
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, urlunparse, unquote
import requests
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if not os.path.exists('user_data'):
    os.makedirs('user_data')

DATA_PATH = "data/dataset-it-profession.csv"
BOT_TOKEN = "YOUR_BOT_TOKEN_HERE"

bot = Bot(token=BOT_TOKEN)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)

class RegistrationStates(StatesGroup):
    waiting_for_full_name = State()
    waiting_for_email = State()
    waiting_for_position = State()
    waiting_for_search_query = State()

class ITEventSemanticSearch:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = None
        self.index = None
        self.model = None
        self.is_initialized = False
        
        try:
            self._initialize()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")
    
    def _initialize(self):
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
        
        self.df = pd.read_csv(self.csv_path, sep=',', encoding='utf-8')
        
        text_columns = ['Event Name', 'Description', 'Category', 'Location']
        for col in text_columns:
            if col in self.df.columns:
                self.df[col] = self.df[col].fillna('').astype(str)
        
        self.df['search_text'] = (
            self.df['Event Name'] + ". " +
            self.df['Description'] + ". " +
            self.df.get('Category', '') + ". " +
            self.df.get('Location', '')
        )
        
        self.model = SentenceTransformer('cointegrated/rubert-tiny2')
        logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        self._build_vector_index()
        self.is_initialized = True
        logger.info("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _build_vector_index(self):
        texts = self.df['search_text'].tolist()
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π...")
        
        embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False)
        
        embeddings = embeddings.astype(np.float32)
        faiss.normalize_L2(embeddings)
        
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(embeddings)
        logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω, –¥–æ–±–∞–≤–ª–µ–Ω–æ {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    
    def search(self, query: str, top_k: int = 5) -> List[str]:
        if not self.is_initialized:
            return ["–°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."]
        
        if not isinstance(query, str) or len(query.strip()) < 2:
            return ["–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–æ–π –Ω–µ –º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤."]
        
        try:
            query = query.strip()
            logger.info(f"–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            
            query_embedding = self.model.encode([query], convert_to_numpy=True)
            query_embedding = query_embedding.astype(np.float32)
            faiss.normalize_L2(query_embedding)
            
            distances, indices = self.index.search(query_embedding, top_k)
            
            results = []
            seen_events = set()
            for i, idx in enumerate(indices[0]):
                if 0 <= idx < len(self.df):
                    event_info = self.df.iloc[idx]['End Date']
                    if event_info and event_info not in seen_events:
                        seen_events.add(event_info)
                        results.append(f"‚Ä¢ {event_info}")
                        if len(results) >= top_k:
                            break
            
            if not results:
                return ["–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É."]
            
            return results[:top_k]
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return ["–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."]

event_search = ITEventSemanticSearch(DATA_PATH)

def is_valid_email(email):
    pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$'
    return re.match(pattern, email) is not None

@dp.message(Command("start"))
async def cmd_start(message: Message, state: FSMContext):
    user_id = message.from_user.id
    
    user_file = f"user_data/user_{user_id}.json"
    
    if os.path.exists(user_file):
        with open(user_file, 'r', encoding='utf-8') as f:
            user_data = json.load(f)
        await message.answer(
            f"–í—ã —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã!\n\n"
            f"–í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ:\n"
            f"–§–ò–û: {user_data['full_name']}\n"
            f"–ü–æ—á—Ç–∞: {user_data['email']}\n"
            f"–î–æ–ª–∂–Ω–æ—Å—Ç—å: {user_data['position']}\n"
            f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID: {user_data['unique_id']}\n\n"
            f"–ß—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /search –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å."
        )
    else:
        await message.answer("–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é.\n\n"
                           "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ –§–ò–û:")
        await state.set_state(RegistrationStates.waiting_for_full_name)

@dp.message(RegistrationStates.waiting_for_full_name)
async def process_full_name(message: Message, state: FSMContext):
    full_name = message.text.strip()
    
    if len(full_name) < 3:
        await message.answer("–§–ò–û —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –§–ò–û:")
        return
    
    await state.update_data(full_name=full_name)
    await message.answer("–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à email:")
    await state.set_state(RegistrationStates.waiting_for_email)

@dp.message(RegistrationStates.waiting_for_email)
async def process_email(message: Message, state: FSMContext):
    email = message.text.strip()
    
    if not is_valid_email(email):
        await message.answer("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π email. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π email:")
        return
    
    await state.update_data(email=email)
    await message.answer("–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à—É –¥–æ–ª–∂–Ω–æ—Å—Ç—å:")
    await state.set_state(RegistrationStates.waiting_for_position)

@dp.message(RegistrationStates.waiting_for_position)
async def process_position(message: Message, state: FSMContext):
    position = message.text.strip()
    
    if len(position) < 2:
        await message.answer("–î–æ–ª–∂–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –¥–æ–ª–∂–Ω–æ—Å—Ç—å:")
        return
    
    data = await state.get_data()
    full_name = data['full_name']
    email = data['email']
    
    unique_id = str(uuid.uuid4())
    
    user_data = {
        'user_id': message.from_user.id,
        'full_name': full_name,
        'email': email,
        'position': position,
        'unique_id': unique_id,
        'registration_date': message.date.isoformat(),
        'username': message.from_user.username if message.from_user.username else None
    }
    
    user_file = f"user_data/user_{message.from_user.id}.json"
    with open(user_file, 'w', encoding='utf-8') as f:
        json.dump(user_data, f, ensure_ascii=False, indent=2)
    
    unique_file = f"user_data/{unique_id}.json"
    with open(unique_file, 'w', encoding='utf-8') as f:
        json.dump(user_data, f, ensure_ascii=False, indent=2)
    
    await state.clear()
    
    welcome_text = (
        f"–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! üéâ\n\n"
        f"–í–∞—à–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\n"
        f"–§–ò–û: {full_name}\n"
        f"–ü–æ—á—Ç–∞: {email}\n"
        f"–î–æ–ª–∂–Ω–æ—Å—Ç—å: {position}\n\n"
        f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID: {unique_id}\n\n"
        f"–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–∫–∞—Ç—å IT-–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è!\n"
        f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /search –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ, –∫–∞–∫–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç.\n\n"
        f"–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:\n"
        f"‚Ä¢ IT –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è –≤ –°–ü–±–ì–£\n"
        f"‚Ä¢ –•–∞–∫–∞—Ç–æ–Ω –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é\n"
        f"‚Ä¢ –í–µ–±–∏–Ω–∞—Ä –ø–æ Python\n"
        f"‚Ä¢ –ú–∏—Ç–∞–ø –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É"
    )
    
    await message.answer(welcome_text)

@dp.message(Command("search"))
async def cmd_search(message: Message, state: FSMContext):
    user_id = message.from_user.id
    user_file = f"user_data/user_{user_id}.json"
    
    if not os.path.exists(user_file):
        await message.answer("–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–Ω–∞—á–∞–ª–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start")
        return
    
    await message.answer("üîç –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ IT-–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π:")
    await state.set_state(RegistrationStates.waiting_for_search_query)

@dp.message()
async def handle_text_search(message: Message, state: FSMContext):
    current_state = await state.get_state()
    
    if current_state == RegistrationStates.waiting_for_search_query:
        query = message.text.strip()
        await state.clear()
        
        user_id = message.from_user.id
        user_file = f"user_data/user_{user_id}.json"
        
        if not os.path.exists(user_file):
            await message.answer("–û—à–∏–±–∫–∞: –≤—ã –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /start")
            return
        
        results = event_search.search(query, top_k=5)
        
        response = f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n\n"
        
        if results and len(results) > 0:
            for i, result in enumerate(results, 1):
                response += f"{i}. {result}\n"
        else:
            response += "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π."
        
        response += "\n\nüîç –ß—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /search –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å."
        
        await message.answer(response)
        return
    
    user_id = message.from_user.id
    user_file = f"user_data/user_{user_id}.json"
    
    if not os.path.exists(user_file):
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã /start")
        return
    
    query = message.text.strip()
    
    if len(query) < 2:
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–æ–π –Ω–µ –º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π.")
        return
    
    results = event_search.search(query, top_k=5)
    
    response = f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n\n"
    
    if results and len(results) > 0:
        for i, result in enumerate(results, 1):
            response += f"{i}. {result}\n"
    else:
        response += "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π."
    
    response += "\n\nüîç –ß—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /search –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å."
    
    await message.answer(response)

@dp.message(Command("mydata"))
async def cmd_mydata(message: Message):
    user_id = message.from_user.id
    user_file = f"user_data/user_{user_id}.json"
    
    if os.path.exists(user_file):
        with open(user_file, 'r', encoding='utf-8') as f:
            user_data = json.load(f)
        
        await message.answer(
            f"–í–∞—à–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n\n"
            f"üë§ –§–ò–û: {user_data['full_name']}\n"
            f"üìß –ü–æ—á—Ç–∞: {user_data['email']}\n"
            f"üíº –î–æ–ª–∂–Ω–æ—Å—Ç—å: {user_data['position']}\n"
            f"üÜî –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID: {user_data['unique_id']}\n"
            f"üìÖ –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {user_data['registration_date'][:10]}"
        )
    else:
        await message.answer("–í—ã –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏.")

@dp.message(Command("help"))
async def cmd_help(message: Message):
    help_text = (
        "–ü–æ–º–æ—â—å –ø–æ –±–æ—Ç—É:\n\n"
        "üöÄ /start - –ù–∞—á–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –∏–ª–∏ –ø–æ–∫–∞–∑–∞—Ç—å –¥–∞–Ω–Ω—ã–µ\n"
        "üîç /search - –ù–∞–π—Ç–∏ IT-–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è\n"
        "üìã /mydata - –ü–æ–∫–∞–∑–∞—Ç—å –≤–∞—à–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n"
        "üÜò /help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\n\n"
        "üí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫:\n"
        "‚Ä¢ –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –≤ —á–∞—Ç\n"
        "‚Ä¢ –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /search\n"
        "‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:\n"
        "  - IT –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è –≤ –°–ü–±–ì–£\n"
        "  - –•–∞–∫–∞—Ç–æ–Ω –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é\n"
        "  - –í–µ–±–∏–Ω–∞—Ä –ø–æ Python\n"
        "  - –ú–∏—Ç–∞–ø –ø–æ AI –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"
    )
    await message.answer(help_text)

@dp.message(Command("reregister"))
async def cmd_reregister(message: Message, state: FSMContext):
    user_id = message.from_user.id
    user_file = f"user_data/user_{user_id}.json"
    
    if os.path.exists(user_file):
        os.remove(user_file)
        await message.answer("–í–∞—à–∏ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —É–¥–∞–ª–µ–Ω—ã. –î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –∑–∞–Ω–æ–≤–æ.\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ –§–ò–û:")
        await state.set_state(RegistrationStates.waiting_for_full_name)
    else:
        await message.answer("–í—ã –µ—â–µ –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏.")

@dp.errors()
async def error_handler(update, exception):
    logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {exception}")
    if update.message:
        await update.message.answer("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
    return True

def get_feed(user_id):
    user_file = f"user_data/user_{user_id}.json"
    query = None
    
    if os.path.exists(user_file):
        with open(user_file, 'r', encoding='utf-8') as f:
            user_data = json.load(f)
        
        status = user_data['position']
        query = f"IT –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –¥–ª—è {status} –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ"

        SEARCH(query, user_id)
        recs_file = f"events{user_id}.json"

        with open(recs_file, 'r', encoding='utf-8') as f:
            recs_data = json.load(f)

async def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    
    if not event_search.is_initialized:
        logger.warning("RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ü–æ–∏—Å–∫ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
    
    await dp.start_polling(bot)

if __name__ == "__main__":
    import asyncio
    logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞...")
    asyncio.run(main())

class DuckDuckGoSearch:
    def __init__(self, cache_size: int = 200, cache_ttl: int = 1800):
        self.cache: OrderedDict = OrderedDict()
        self.cache_ttl = cache_ttl
        self.cache_timestamps: Dict[str, datetime] = {}
        self.cache_size = cache_size
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://duckduckgo.com/',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def _generate_cache_key(self, query: str) -> str:
        salt = "ddg_v2"
        normalized = re.sub(r'\s+', ' ', query.lower().strip())
        return hashlib.sha256(f"{normalized}{salt}".encode('utf-8')).hexdigest()
    
    def _is_ru_domain(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Å–∞–π—Ç –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ RU —Å–µ–≥–º–µ–Ω—Ç—É"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            allowed = ['.ru', '.su', '.—Ä—Ñ', '.moscow', '.tech', '.com', '.org', '.net']
            return any(domain.endswith(tld) or f".{tld}" in domain for tld in allowed)
        except:
            return False

    def _clean_url(self, raw_url: str) -> Optional[str]:
        try:
            if 'duckduckgo.com/l/?uddg=' in raw_url:
                match = re.search(r'uddg=([^&]+)', raw_url)
                if match:
                    raw_url = unquote(match.group(1))
            
            raw_url = raw_url.strip()
            if not raw_url.startswith(('http://', 'https://')):
                return None
                
            parsed = urlparse(raw_url)
            if not parsed.netloc:
                return None
                
            if 'duckduckgo' in parsed.netloc or 'yandex' in parsed.netloc or 'google' in parsed.netloc:
                return None
                
            return raw_url
        except:
            return None
    
    def search(self, query: str) -> List[str]:
        cache_key = self._generate_cache_key(query)
        if cache_key in self.cache and (datetime.now() - self.cache_timestamps.get(cache_key, datetime.min)).total_seconds() < self.cache_ttl:
            return self.cache[cache_key]

        url = "https://html.duckduckgo.com/html/"
        data = {'q': query, 'kl': 'ru-ru', 'df': 'y'}


        try:
            resp = self.session.post(url, data=data, timeout=15)
            resp.raise_for_status()
            
            urls = []
            seen = set()
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            links = soup.find_all('a', class_='result__a')
            
            for link_tag in links:
                href = link_tag.get('href')
                if not href: continue
                
                clean = self._clean_url(href)
                if clean and clean not in seen:
                    if self._is_ru_domain(clean):
                        urls.append(clean)
                        seen.add(clean)
            
            if not urls:
                raw_links = re.findall(r'href=["\'](https?://[^"\']+)["\']', resp.text)
                for href in raw_links:
                    clean = self._clean_url(href)
                    if clean and clean not in seen and self._is_ru_domain(clean):
                        urls.append(clean)
                        seen.add(clean)

            self.cache[cache_key] = urls[:10]
            self.cache_timestamps[cache_key] = datetime.now()
            return urls[:10]

        except Exception as e:
            print(f"Search failed: {e}")
            return []

class EventParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; EventBot/1.0)',
            'Accept-Language': 'ru-RU,ru;q=0.9',
        })
        
        self.months = {
            '—è–Ω–≤': 1, '—è–Ω–≤–∞—Ä—è': 1, '—è–Ω–≤–∞—Ä—å': 1,
            '—Ñ–µ–≤': 2, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '—Ñ–µ–≤—Ä–∞–ª—å': 2,
            '–º–∞—Ä': 3, '–º–∞—Ä—Ç–∞': 3, '–º–∞—Ä—Ç': 3,
            '–∞–ø—Ä': 4, '–∞–ø—Ä–µ–ª—è': 4, '–∞–ø—Ä–µ–ª—å': 4,
            '–º–∞—è': 5, '–º–∞–π': 5,
            '–∏—é–Ω': 6, '–∏—é–Ω—è': 6, '–∏—é–Ω—å': 6,
            '–∏—é–ª': 7, '–∏—é–ª—è': 7, '–∏—é–ª—å': 7,
            '–∞–≤–≥': 8, '–∞–≤–≥—É—Å—Ç–∞': 8, '–∞–≤–≥—É—Å—Ç': 8,
            '—Å–µ–Ω': 9, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '—Å–µ–Ω—Ç—è–±—Ä—å': 9,
            '–æ–∫—Ç': 10, '–æ–∫—Ç—è–±—Ä—è': 10, '–æ–∫—Ç—è–±—Ä—å': 10,
            '–Ω–æ—è': 11, '–Ω–æ—è–±—Ä—è': 11, '–Ω–æ—è–±—Ä—å': 11,
            '–¥–µ–∫': 12, '–¥–µ–∫–∞–±—Ä—è': 12, '–¥–µ–∫–∞–±—Ä—å': 12,
        }

    def get_soup(self, url: str) -> Optional[BeautifulSoup]:
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            if response.encoding is None:
                response.encoding = 'utf-8'
            return BeautifulSoup(response.text, 'html.parser')
        except Exception:
            return None

    def _get_meta(self, soup: BeautifulSoup, attrs_list: List[Dict]) -> str:
        for attrs in attrs_list:
            tag = soup.find('meta', attrs=attrs)
            if tag and tag.get('content'):
                return tag['content'].strip()
        return ""

    def _extract_text_by_keyword(self, soup: BeautifulSoup, keywords: List[str], length=300) -> str:
        body_text = soup.get_text(" ", strip=True)
        for kw in keywords:
            if kw.lower() in body_text.lower():
                idx = body_text.lower().find(kw.lower())
                start = max(0, idx)
                end = min(len(body_text), idx + length)
                return body_text[start:end].strip() + "..."
        return ""

    def _parse_date(self, date_str: str, year: str = None) -> Optional[datetime]:
        try:
            match = re.search(r'(\d{1,2})\s+([–∞-—è–ê-–Ø]+)\s+(\d{4})', date_str)
            if match:
                day = int(match.group(1))
                month_str = match.group(2).lower()
                year = int(match.group(3))
                
                for key, month in self.months.items():
                    if key in month_str:
                        return datetime(year, month, day)
            
            match = re.search(r'(\d{1,2})\s+([–∞-—è–ê-–Ø]+)', date_str)
            if match and year:
                day = int(match.group(1))
                month_str = match.group(2).lower()
                year_int = int(year)
                
                for key, month in self.months.items():
                    if key in month_str:
                        return datetime(year_int, month, day)
            
            match = re.search(r'(\d{1,2})\.(\d{1,2})\.(\d{4})', date_str)
            if match:
                day, month, year = int(match.group(1)), int(match.group(2)), int(match.group(3))
                return datetime(year, month, day)
                
        except (ValueError, AttributeError):
            pass
        
        return None

    def _is_valid_title(self, title: str) -> bool:
        if not title or len(title) < 5:
            return False
        
        has_cyrillic = bool(re.search('[–∞-—è–ê-–Ø—ë–Å]', title))
        has_latin = bool(re.search('[a-zA-Z]', title))
        
        has_garbage = bool(re.search(r'[√ê√ë√ê]{3,}', title))
        
        return (has_cyrillic or has_latin) and not has_garbage

    def parse(self, url: str) -> Dict[str, Any]:
        soup = self.get_soup(url)
        data = {k: '' for k in ['Year', 'Start Date', 'End Date', 'Event Name', 'Event Type', 
                                'Description', 'Participants Count', 'Speakers/Organizers', 
                                'Partners', 'Category', 'Location', 'Source URL', 'Parsed Date']}
        data['Source URL'] = url
        
        if not soup:
            return data

        data['Event Name'] = self._get_meta(soup, [
            {'property': 'og:title'}, {'name': 'twitter:title'}, {'name': 'title'}
        ])
        if not data['Event Name'] and soup.title:
            data['Event Name'] = soup.title.string or ''

        if not self._is_valid_title(data['Event Name']):
            return data 

        data['Description'] = self._get_meta(soup, [
            {'property': 'og:description'}, {'name': 'description'}
        ])

        data['Location'] = self._get_meta(soup, [{'property': 'og:locality'}])
        if not data['Location']:
            text = soup.get_text()
            if "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥" in text or "–°–ü–±" in text or "–ü–∏—Ç–µ—Ä" in text:
                data['Location'] = "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"

        text = soup.get_text(" ", strip=True)
        
        year_match = re.search(r'202[4-9]', text)
        if year_match:
            data['Year'] = year_match.group(0)
        else:
            data['Year'] = str(datetime.now().year)
        
        date_patterns = [
            r'(\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞—è|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*\s+\d{4})',
            r'(\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞—è|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*)',
            r'(\d{1,2}\.\d{1,2}\.\d{4})'
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, text.lower())
            if date_match:
                date_str = date_match.group(0)
                data['Start Date'] = date_str
                
                parsed_date = self._parse_date(date_str, data['Year'])
                if parsed_date:
                    data['Parsed Date'] = parsed_date
                break

        data['Speakers/Organizers'] = self._extract_text_by_keyword(soup, ['–°–ø–∏–∫–µ—Ä—ã', 'Speakers', '–î–æ–∫–ª–∞–¥—á–∏–∫–∏', '–í–µ–¥—É—â–∏–µ'])
        data['Partners'] = self._extract_text_by_keyword(soup, ['–ü–∞—Ä—Ç–Ω–µ—Ä—ã', '–°–ø–æ–Ω—Å–æ—Ä—ã', 'Partners'])

        title_lower = (data['Event Name'] or '').lower()
        if '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü' in title_lower: data['Event Type'] = '–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è'
        elif '–º–∏—Ç–∞–ø' in title_lower or 'meetup' in title_lower: data['Event Type'] = '–ú–∏—Ç–∞–ø'
        elif '—Ö–∞–∫–∞—Ç–æ–Ω' in title_lower: data['Event Type'] = '–•–∞–∫–∞—Ç–æ–Ω'
        else: data['Event Type'] = '–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'

        return data

class JsonWriter:
    def __init__(self, filename: str = 'events.json'):
        self.filename = filename
        self.data = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å
        if os.path.exists(self.filename):
            try:
                with open(self.filename, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)
                    if not isinstance(self.data, list):
                        self.data = []
            except (json.JSONDecodeError, Exception):
                self.data = []
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ñ–∞–π–ª {self.filename} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç. –°–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π.")
    
    def append(self, row: Dict):
        clean_row = {}
        for k, v in row.items():
            if k == 'Parsed Date':
                continue  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
            if isinstance(v, datetime):
                clean_row[k] = v.strftime('%Y-%m-%d')
            else:
                clean_row[k] = str(v).strip() if v else ''
        self.data.append(clean_row)
    
    def save(self):
        with open(self.filename, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        print(f"\n–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.filename} ({len(self.data)} –∑–∞–ø–∏—Å–µ–π)")

def SEARCH(query, user_id):
    today = datetime.now()

    searcher = DuckDuckGoSearch()
    urls = searcher.search(query)
    
    if not urls:
        print("–°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(urls)}")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    
    parser = EventParser()
    writer = JsonWriter(f'events{user_id}.json')
    
    added_count = 0
    skipped_count = 0
    
    for i, url in enumerate(urls, 1):
        try:
            print(f"[{i}/{len(urls)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")
            event_data = parser.parse(url)
            
            if not event_data['Event Name'] or not parser._is_valid_title(event_data['Event Name']):
                print(f"  ‚ö† –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã")
                skipped_count += 1
                continue
            
            parsed_date = event_data.get('Parsed Date')
            if parsed_date:
                if parsed_date < today:
                    print(f"–ü—Ä–æ–ø—É—Å–∫: —Å–æ–±—ã—Ç–∏–µ –ø—Ä–æ—à–ª–æ ({parsed_date.strftime('%d.%m.%Y')})")
                    skipped_count += 1
                    continue
                else:
                    print(f"  üìÖ –î–∞—Ç–∞: {parsed_date.strftime('%d.%m.%Y')}")
            else:
                print(f"–î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º")
            
            writer.append(event_data)
            title_display = event_data['Event Name'][:60]
            print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {title_display}...")
            added_count += 1
            
            time.sleep(1)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")
            skipped_count += 1
    writer.save()
